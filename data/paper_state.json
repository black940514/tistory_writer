{
  "current_index": 58,
  "reviewed_papers": [
    "Deep Residual Learning for Image Recognition_2015",
    "ImageNet Classification with Deep Convolutional Neural Networks_2012",
    "Attention Is All You Need_2017",
    "The Elements of Statistical Learning: Data Mining, Inference, and Prediction_2001",
    "Random Forests_2001",
    "Adam: A Method for Stochastic Optimization_2015",
    "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding_2018",
    "Deep Learning_2016",
    "Reinforcement Learning: An Introduction_2018",
    "Pattern Recognition and Machine Learning_2006",
    "Very Deep Convolutional Networks for Large-Scale Image Recognition_2014",
    "Generative Adversarial Nets_2014",
    "Auto-Encoding Variational Bayes_2013",
    "An Introduction to Statistical Learning: With Applications in R_2013",
    "Support-Vector Networks_1995",
    "The Nature of Statistical Learning Theory_1995",
    "Scikit-Learn: Machine Learning in Python_2011",
    "Machine Learning: A Probabilistic Perspective_2012",
    "Greedy Function Approximation: A Gradient Boosting Machine_2001",
    "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift_2015",
    "t-SNE: Visualizing Data using t-SNE_2008",
    "A Survey of Deep Learning_2015",
    "Bagging Predictors_1996",
    "K-means Clustering_1967",
    "XGBoost: A Scalable Tree Boosting System_2016",
    "Language Models are Few-Shot Learners_2020",
    "Microsoft COCO: Common Objects in Context_2014",
    "Dropout: A Simple Way to Prevent Neural Networks from Overfitting_2014",
    "The Matrix Cookbook_2012",
    "Understanding Machine Learning: From Theory to Algorithms_2014",
    "Principal Component Analysis_1933",
    "CLIP: Learning Transferable Visual Models From Natural Language Supervision_2021",
    "Learning Transferable Visual Models From Natural Language Supervision_2021",
    "RoBERTa: A Robustly Optimized BERT Pretraining Approach_2019",
    "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems_2017",
    "Introduction to Machine Learning_2004",
    "SimCLR: A Simple Framework for Contrastive Learning of Visual Representations_2020",
    "GPT-2: Language Models are Unsupervised Multitask Learners_2019",
    "Introduction to Machine Learning with Python: A Guide for Data Scientists_2016",
    "A Tutorial on Support Vector Machines for Pattern Recognition_1998",
    "DETR: End-to-End Object Detection with Transformers_2020",
    "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension_2019",
    "A Few Useful Things to Know about Machine Learning_2012",
    "NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis_2020",
    "Stable Diffusion: High-Resolution Image Synthesis with Latent Diffusion Models_2022",
    "Latent Diffusion Models for High-Resolution Image Synthesis_2022",
    "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale_2020",
    "Improving Language Understanding by Generative Pre-Training_2018",
    "UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction_2018",
    "Deep Learning with Python_2017",
    "A Tutorial on Principal Component Analysis_2014",
    "Training language models to follow instructions with human feedback_2022",
    "LightGBM: A Highly Efficient Gradient Boosting Decision Tree_2017",
    "Swin Transformer: Hierarchical Vision Transformer using Shifted Windows_2021",
    "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer_2020",
    "LLaMA: Open and Efficient Foundation Language Models_2023",
    "Python Machine Learning_2015",
    "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks_2020"
  ]
}